{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Fix the model's understanding of fingers."
      ],
      "metadata": {
        "id": "nPBJr-v2DKWP"
      },
      "id": "nPBJr-v2DKWP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Video: Fix the issues of objects suddenly disappearing or changing color."
      ],
      "metadata": {
        "id": "0C2AhS_vDSGe"
      },
      "id": "0C2AhS_vDSGe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Make the model understand stuff like \"Empire State Building on it's side\" which it currently doesn't understand."
      ],
      "metadata": {
        "id": "OVjjdd4qDhol"
      },
      "id": "OVjjdd4qDhol"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Conditional stable diffusion and image-to-image take text &/or an image as input. But what other modalities can be taken as input? Sound? (e.g., music / a bird call) / speech (in particular the emotion & gender / accent aspects of voice - imagine creating a talking video character (realistic / cartoony) based on voice only! - you can create a video radio station / podcast from speech alone; the lip movements should be based on the text, target language & time alignment; use CLIP for audio encoding?) Tabular data? (e.g., generate visualizations from an Excel sheet)"
      ],
      "metadata": {
        "id": "v3JAnXp20gtw"
      },
      "id": "v3JAnXp20gtw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. An image captioning model uses an image encoder and an autoregressive text decoder. Is there any equivalent to the next token prediction task in computer vision? Next pixel prediction? Next patch prediction? The latter may be possible. If so, then is it possible to replace diffusion models with the analogue of image captioning models, i.e., encode the text, and use an auto-regressive decoder to generate the image? How about using diffusion to generate the initial patch, and then using auto-regression to generate the rest? Then we can create a Midjourney-like zoom out feature."
      ],
      "metadata": {
        "id": "uAbrjpui_3D-"
      },
      "id": "uAbrjpui_3D-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Have the concepts of guidance scale, negative prompt, sound-to-sound, textual inversion & DreamBooth been implemented for audio stable diffusion / other modalities? If not, they may be opportunities, especially sound-to-sound."
      ],
      "metadata": {
        "id": "GG6sX1Rpyp96"
      },
      "id": "GG6sX1Rpyp96"
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Not related to stable diffusion: Has a high-quality audio captioning model been created? This should work similarly to image captioning."
      ],
      "metadata": {
        "id": "w7pBAgh60IcP"
      },
      "id": "w7pBAgh60IcP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. The textual inversion trick of fine-tuning a single token: Can it not be used for models like T5 / Donut to teach them new concepts (without having to fine-tune the full model)? Try it out on T5 and Donut to add new tasks / keys to an existing fine-tuned model! Compare the results to a stage-1 model fine-tuned on the full dataset with all the tasks / keys. Explore how this is related to machine unlearning: Can it be used to forget just a single task / key? The latter may be a new type of unlearning. So far, unlearning has focused on forgetting training examples, but not full concepts. (Perhaps just resetting the embedding vector for that task / key special token to random values will do the trick?)"
      ],
      "metadata": {
        "id": "ZffwKfYH3t83"
      },
      "id": "ZffwKfYH3t83"
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. A one-shot style transfer model. Input a new image and a reference image (containing a style), and the model should be able to perform style transfer. Does it already exist? This is as opposed to full pre-training / fine-tuning (on a particular style without prompting, or a limited set of styles with prompting) / textual inversion approaches."
      ],
      "metadata": {
        "id": "wAsWC_2gAJ6z"
      },
      "id": "wAsWC_2gAJ6z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Research ideas related to DreamBooth mentioned in Lesson 9:\n",
        "\n",
        "> Other ideas that may work include: use Exponential Moving Average (EMA) so that the final weights preserve some of the previous knowledge, use progressive learning rates for fine-tuning, or combine the best of Textual Inversion with DreamBooth. These could make for some interesting projects to try out!\n",
        "\n",
        "These are related to prior preservation."
      ],
      "metadata": {
        "id": "CugmMUOppWTq"
      },
      "id": "CugmMUOppWTq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Jeremy's formulation of Stable Diffusion: According to him, it can take you in some innovative research directions. What are they?"
      ],
      "metadata": {
        "id": "o04mZIuvTBfV"
      },
      "id": "o04mZIuvTBfV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **Repetition of a point in 'Lesson_9_Notes.ipynb' because it's a research idea:** The first function (`f`) he mentions: it outputs the probability that an image is a handwritten digit. Why not just create a binary classification model (with a single output unit & a sigmoid activation function)? The model can be trained using MNIST (handwritten digits) & Fashion-MNIST (not handwritten digits)! According to Jeremy, if we have this function, we can actually use it to generate handwritten digits. **Note:** With the MNIST, Fashion-MNIST approach, you don't have to start with random noise to perform gradient ascent."
      ],
      "metadata": {
        "id": "XxYau3YPXif2"
      },
      "id": "XxYau3YPXif2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **Repetition of a point in 'Lesson_9_Notes.ipynb' because it's a research idea:** For `f.backward()` and `X_3.grad` to work, we need `X_3` to have `requires_grad=True`. But `X_3` is an image (an input tensor with `requires_grad=False`), not an `nn.Parameter` with `requires_grad=True`. So how would that work? A hack might be: after the model is trained, freeze all it's parameters. Then set `requires_grad=True` for the input tensor. And do a forward pass without using the `torch.no_grad()` context manager. Then you might be able to get the gradient of `f` w.r.t. `X_3`? **Note:** This idea of starting from an input, and changing it (using gradient ascent) till it becomes something else might be more general than just the image generation task. What other tasks could we use this strategy for? Accent softening? Music enhancement (e.g., adding more instruments)? Style transfer? What else? (First, we have to validate that this strategy works using the MNIST dataset.)"
      ],
      "metadata": {
        "id": "CItB3GdWZjI1"
      },
      "id": "CItB3GdWZjI1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. A potential alternative to using a UNet to predict noise: Maybe generate 784 length vector labels for amount of noise added to an image, and train an MLP regression model to predict that?"
      ],
      "metadata": {
        "id": "ATGfYSazY9y3"
      },
      "id": "ATGfYSazY9y3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Apart from Stable Diffusion, in what other situations is training a large neural net prohibitively expensive? Video data? Can we use a VAE to compress the data in such cases? Can this be used to perform modeling experiments that are currently only accessible to the largest AGI labs due to compute limitations?"
      ],
      "metadata": {
        "id": "6sxfyDQppjqe"
      },
      "id": "6sxfyDQppjqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. The idea of contrastive loss - it is surely being used in audio applications as well, such as zero shot audio classification and audio diffusion. What other audio applications does contrastive loss have? What about other modalities such as video (GIFs for example), tabular data, graphs & charts, etc? What are the potential applications of putting two different modalities into the same space?"
      ],
      "metadata": {
        "id": "QyndUfMURX4p"
      },
      "id": "QyndUfMURX4p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Related to above: Is text the only form of guidance possible? How about reversing the process? That it, providing the model an image, and asking it to generate some text / audio / video data. (This is already done for image captioning. But the idea is: can we do diffusion? What would the inputs & outputs of such a model look like?) How about an audio file as guidance for audio stable diffusion? (This is not about creating an audio-to-audio pipeline. This is about using an input audio as guidance. For example, in the case of accent conversion.)"
      ],
      "metadata": {
        "id": "A6vJZQwknAZR"
      },
      "id": "A6vJZQwknAZR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **Repetition of a point in 'Lesson_9_Notes.ipynb' because it's a research idea:** Questions like how to choose the value of the constant `c` are decided by the \"*diffusion sampler*\". But this looks a lot like deep learning optimizers (Momentum, RMSProp, Adam, etc). In a deep learning optimizer, the constant `c` is the learning rate. So concepts such as momentum should be applicable to diffusion samplers as well! This is an area of research that Jeremy is exploring. Diffusion models originally came from the world of differential equations. And there are a whole lot of parallel concepts in the two worlds of (a) optimizers and (b) differential equations. And so, differential equation solvers use a lot of the same kinds of ideas that deep learning optimizers use. One thing that differential equation solvers do is that they tend to take '*time*' as an input. And in fact, pretty much all diffusion models take not just the noisy latent and the prompt as an input; they also take '*time*' as an input. The idea is: the model will be better at removing the noise if you tell it how much noise still exists (after removing noise in the previous time steps). Jeremy very strongly suspects that this premise is incorrect, because figuring out how noisy an image is should be very straightforward for a fancy neural net. (**Idea:** Train a separate regression model to do this, i.e., predict how much noise there is. Or maybe use an auxilliary target in the same neural net.) So Jeremy very much doubts that we actually need to pass in '*time*' as an input. And as soon as you stop doing that, things stop looking like differential equations, and they start looking like optimizers."
      ],
      "metadata": {
        "id": "LIRUYk018GIC"
      },
      "id": "LIRUYk018GIC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **Repetition of a point in 'Lesson_9_Notes.ipynb' because it's a research idea:** We decided that the loss function of the UNet is MSE. The truth is, in statistics & ML, every time you see somebody use MSE, it's because the math worked out easier that way. What if we replaced MSE with more sophisticated loss functions like \"*perceptual loss*\"? This loss function tells us: after removing noise from a noisy latent, how good is the noisy latent? Does it look like a (compressed) digit? Does it have the qualities of a (compressed) digit?"
      ],
      "metadata": {
        "id": "ske_J0ca9tAL"
      },
      "id": "ske_J0ca9tAL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Self-supervised learning: (i) In vision, these are the current types: https://www.fast.ai/posts/2020-01-13-self_supervised.html The question: Are there better self-supervised learning tasks that you can envision? The efficacy can be measured based on performance on downstream tasks. (ii) In NLP, what else can be there apart from next token prediction, fill mask and next sentence prediction? If you can invent a new type, you can change language models forever. Examine the above vision examples for some ideas. Even if it doesn't replace next token prediction, it could become an auxilliary task that improves the performance of language models."
      ],
      "metadata": {
        "id": "JaNZZBQB5hup"
      },
      "id": "JaNZZBQB5hup"
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Can self-supervised learning be used to pre-train a CV model for specialized images for which ImageNet pre-trained models aren't useful? Examples could be certain types of medical images. Can publish high quality pre-trained models for medical imaging using this approach. Do this!"
      ],
      "metadata": {
        "id": "-QcmoGbr6SfU"
      },
      "id": "-QcmoGbr6SfU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What tasks are the current pre-trained models for audio trained on? What does self-supervised learning for audio look like? What about for tabular data?"
      ],
      "metadata": {
        "id": "jiKMR1HN6ud3"
      },
      "id": "jiKMR1HN6ud3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Are there causal / masked LMs for audio? If so, do we perform domain adaptation before fine-tuning? Both of these questions have potential to turn into papers."
      ],
      "metadata": {
        "id": "rhar10aaWQRH"
      },
      "id": "rhar10aaWQRH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. RLHF / DPO can also be used to optimize for the level of disharmony (given the right type of labeled data). Furthermore, you could optimize one model for agreeableness and another model for disagreeableness. These could then be released upon social media platforms to wreak havoc. This is a good research area under AI safety."
      ],
      "metadata": {
        "id": "FcoMqSLQUdV0"
      },
      "id": "FcoMqSLQUdV0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Train a causal speech language model using a modified version of AWD_LSTM and an audio version of Jeremy's human numbers dataset. Do a literature review on what kinds of language / self-supervised pre-trained models are available for audio."
      ],
      "metadata": {
        "id": "7Zzp_bDuHx7Z"
      },
      "id": "7Zzp_bDuHx7Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a speech-to-speech translation model using an audio version of Jeremy's human numbers dataset. Use TTS to create the dataset."
      ],
      "metadata": {
        "id": "kP4LPdiUmRci"
      },
      "id": "kP4LPdiUmRci"
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train the above model with time alignment. Then try (i) background music preservation and (ii) background noise removal."
      ],
      "metadata": {
        "id": "kJ8ONX43v7SI"
      },
      "id": "kJ8ONX43v7SI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Come up with a logic gate for prompt-based task choosing for computer vision (after studying LSTM and GRU)."
      ],
      "metadata": {
        "id": "3GTPRLveBcmq"
      },
      "id": "3GTPRLveBcmq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. **Exercise:** Pre-train the `RNN-T` (Conformer & Emformer), Translatotron 2 and other RNN based audio architectures (after 14th October 2024)."
      ],
      "metadata": {
        "id": "QdyJ34ZQI1tO"
      },
      "id": "QdyJ34ZQI1tO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. **To answer after studying the transformer architecture:** Does a backwards model make sense for transformer-based models? Maybe it only makes sense for causal LMs if we chop off the head and re-use the body for classification and other similar tasks. But maybe it doesn't make sense for masked LMs because they use bidirectional attention anyway."
      ],
      "metadata": {
        "id": "f5TuEnoQiMOb"
      },
      "id": "f5TuEnoQiMOb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Fine-tune an LLM using the GAN approach to trick AI text detectors to think that the text was written by a human."
      ],
      "metadata": {
        "id": "rTLs5zwflqIJ"
      },
      "id": "rTLs5zwflqIJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Try to implement `RandomCrop` for NLP:\n",
        "\n",
        "> ...data augmentation hasn’t been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!).\n",
        "\n",
        "But before this, check whether it already exists in nlpaug & textattack."
      ],
      "metadata": {
        "id": "l3kMDX8lyeps"
      },
      "id": "l3kMDX8lyeps"
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. **Related:** Dynamically resize images for collation:\n",
        "\n",
        "> It is possible to do something similar (i.e., `SortedDL` with dynamic padding / squishing / cropping) with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren’t any papers covering it."
      ],
      "metadata": {
        "id": "jiUgVqI4zX8H"
      },
      "id": "jiUgVqI4zX8H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Is it possible to explicitly specify a constraint for relationship between the main output & the aux output? For example, in Ch 10 of Aurelien Geron's book, both are equal. And in Mask R-CNN, the mask lies inside the bounding box. Perhaps a constraint makes sense for time alignment of the movie dubbing problem."
      ],
      "metadata": {
        "id": "3ssXwoGUKSpr"
      },
      "id": "3ssXwoGUKSpr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. F n = F n−1 +F n−2\n",
        "\n",
        "This is called a recurrence relation - it's an equation that connects the terms together.\n",
        "\n",
        "**Idea:** Instead of an RNN using the hidden state and the input at the current time step, can it use the hidden state, the input at the current time step AND the input at the previous time step? How about the previous two time steps? How about skipping a time step when looking back?"
      ],
      "metadata": {
        "id": "e-umpRJQAmyO"
      },
      "id": "e-umpRJQAmyO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. In a seq2seq architecture, the decoder uses the entire encoder hidden state when predicting each new token. But here's a point: When performing translation (say), the decoder should pay attention to the first part of the encoder hidden state when predicting the first few tokens. And it should pay attention to the last part of the encoder hidden state when predicting the last few tokens. Is this happening already? Or can we use a separate recurrence relation / attention layer to do this?"
      ],
      "metadata": {
        "id": "7jiaBBU9jsuv"
      },
      "id": "7jiaBBU9jsuv"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KBH4Z4xfPCpP"
      },
      "id": "KBH4Z4xfPCpP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}